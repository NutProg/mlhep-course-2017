{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Study of  $D^0 \\to K^- \\pi^+ $ decay\n",
    "\n",
    "- Mass\n",
    "- Lifetime\n",
    "\n",
    "\n",
    "<img src=\"http://lhcb-public.web.cern.ch/lhcb-public/en/LHCb-outreach/masterclasses/en/VertexD.png\" width=60%>\n",
    "\n",
    "http://lhcb-public.web.cern.ch/lhcb-public/en/LHCb-outreach/masterclasses/en/D0Lifetime.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "pip install iminuit probfit seaborn cython"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pylab as plt\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn\n",
    "import iminuit, probfit\n",
    "from __future__ import print_function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "if [ ! -f masterclass_all.csv.gz ] ; then\n",
    "  curl -L -o masterclass_all.csv.gz https://www.dropbox.com/s/57i1ulucgeqhjmz/masterclass_all.csv.gz?dl=0\n",
    "fi\n",
    "ls -l masterclass_all.csv.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#df = pd.DataFrame(root_numpy.root2array(datafile_2012, treename='DecayTree'))\n",
    "df0 = pd.read_csv(\"masterclass_all.csv.gz\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print (\"Size:\" , df0.shape)\n",
    "df0.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.hist(df0['D0_MM'], bins=100, histtype='stepfilled', alpha=0.3);\n",
    "plt.xlabel(\"Mass, MeV\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preselection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df = df0[df0['D0_MINIP'] < 2][df0['Kplus_ProbNNk']> 0.2][df0['piminus_ProbNNpi']> 0.2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print (\"Size:\" , df0.shape)\n",
    "plt.hist(df['D0_MM'], bins=100, histtype='stepfilled', alpha=0.3);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## D0 mass measurements\n",
    "\n",
    "* Fitting the D0 mass\n",
    "    - First plot the D0 mass distribution\n",
    "    - Now fit to it. In this fit, leave the signal and mass ranges to their default values.\n",
    "    - Define the signal mass region as +-3 sigma around the mean value\n",
    "    - What is the signal significance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from iminuit import Minuit\n",
    "from probfit import UnbinnedLH, gaussian, linear"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fit_range = (1830, 1900)\n",
    "normalized_poly = probfit.Normalized(probfit.Polynomial(1), fit_range)\n",
    "normalized_poly = probfit.Extended(normalized_poly, extname='NBkg')\n",
    "\n",
    "gauss1 = probfit.Extended(probfit.rename(probfit.gaussian, ['x', 'mu1', 'sigma1']), \n",
    "                          extname='N1')\n",
    "\n",
    "# Define an extended PDF consisting of three components\n",
    "pdf = probfit.AddPdf(normalized_poly, gauss1)\n",
    "\n",
    "print('normalized_poly: {}'.format(probfit.describe(normalized_poly)))\n",
    "print('gauss1:          {}'.format(probfit.describe(gauss1)))\n",
    "print('pdf:             {}'.format(probfit.describe(pdf)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "binned_likelihood = probfit.BinnedLH(pdf, df['D0_MM'], bins=200, extended=True, bound=fit_range)\n",
    "\n",
    "# This is a quite complex fit (11 free parameters!), so we need good starting values.\n",
    "# Actually we even need to set an initial parameter error\n",
    "# for 'mu1' and 'mu2' to make MIGRAD converge.\n",
    "# The initial parameter error is used as the initial step size in the minimization.\n",
    "pars = dict(mu1=1865, sigma1=10, N1=35000,\n",
    "            c_0=0.1, c_1=0.01, NBkg=20000)\n",
    "minuit = iminuit.Minuit(binned_likelihood, pedantic=False, print_level=0, **pars)\n",
    "# You can see that the model already roughly matches the data\n",
    "binned_likelihood.draw(minuit, parts=True);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "minuit.migrad();\n",
    "#minuit.minos();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "binned_likelihood.show(minuit, parts=True);\n",
    "minuit.print_fmin()\n",
    "minuit.print_matrix()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print (\"D0 mass:\", minuit.values['mu1'], \"+/-\", minuit.errors['mu1'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## D0 lifetime measurement\n",
    "\n",
    "Welcome to the LHCb masterclass exercise on measuring the lifetime of the D0 meson. \n",
    "The goal of this exercise is to measure the lifetime of the D0 meson, a fundamental\n",
    "particle made of a charm quark and an up anti-quark. In order to do so, you will\n",
    "first learn how to separate signal D0 mesons from backgrounds. Finally, you will\n",
    "compare your results to the values found by the Particle Data Group (http://pdgLive.lbl.gov).\n",
    "\n",
    "#### Step-by-step instructions :\n",
    "\n",
    "3. Plot the variable distributions. You will see three further plots appearing, and\n",
    "in each one the blue points represent the distribution of the signal in that variable\n",
    "while the red points represent the distribution of the background. The plot is logarithmic\n",
    "in the Y axis, and each point represents the fraction of the total signal in that bin.\n",
    "Which regions of each variable contain mostly signal? Which contain mostly background?\n",
    "4. Fit the lifetime distribution. Save the results\n",
    "of your fit and compare them to the PDG value. Do they agree?\n",
    "5. Repeat step 4 but now varying the upper D0 log(IP) variable range\n",
    "from 1.5 to -2 in steps of 0.2. Do you notice a pattern?\n",
    "6. Does the D0 lifetime with an log(IP) cut of\n",
    "-1.5 agree better or worse with the PDG than the lifetime with an log(IP) cut of 1.5?;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lifetime\n",
    "\n",
    "Exponential decay:\n",
    "\n",
    "$$\\frac{dN}{dt} = -\\lambda N$$\n",
    "\n",
    "where $\\lambda$ is decay rate, or \n",
    "\n",
    "$$\\frac{dN}{N} = -\\lambda dt$$\n",
    "\n",
    "integrating:\n",
    "\n",
    "$$\\mathrm{ln} N = - \\lambda t + C$$\n",
    "$$ N(t) = e^Ce^{-\\lambda t} = N_0 e^{-\\lambda t} $$\n",
    "\n",
    "Mean lifetime (https://en.wikipedia.org/wiki/Exponential_decay#Derivation_of_the_mean_lifetime):\n",
    "\n",
    "$$\\tau = \\langle t \\rangle = \\int_0^\\infty t \\cdot c \\cdot N_0 e^{-\\lambda t}\\, dt = \\int_0^\\infty \\lambda t e^{-\\lambda t}\\, dt = \\frac{1}{\\lambda}$$\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Variables distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(18, 6))\n",
    "plt.subplot(1,3,1)\n",
    "plt.hist(df['D0_TAU']*1000, bins=50, histtype='stepfilled', alpha=0.5)\n",
    "plt.yscale('log', basey=10)\n",
    "plt.xlabel('t, ps')\n",
    "plt.ylabel('log10(N)')\n",
    "plt.title('D0_TAU, Decay time')\n",
    "\n",
    "plt.subplot(1,3,2)\n",
    "plt.hist(np.log10(df['D0_MINIP']), bins=50, histtype='stepfilled', alpha=0.5);\n",
    "plt.yscale('log')\n",
    "plt.title('log10(D0_MINIP)')\n",
    "plt.xlabel('log10(MINIP)')\n",
    "\n",
    "plt.subplot(1,3,3)\n",
    "plt.hist(df['D0_MM'], bins=50, histtype='stepfilled', alpha=0.5);\n",
    "plt.xlabel('MeV')\n",
    "plt.yscale('log')\n",
    "plt.title('D0_MM')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Background substraction\n",
    "\n",
    "idea: \n",
    "\n",
    "1. We are interested in signal/background sample distribution (not individual events) wrt variable TAU.\n",
    "2. Check that variable MM is not correlated with TAU.\n",
    "3. For the whole sample define signal region R_s wrt mass variable (MM), i.e. interval with majority of signal. It splits whole data sample into 2 subsamples: \"signal region events\" and \"sideband region events\".\n",
    "4. Since TAU and MM are not correlated, distribution of a variable TAU for background to be the distribution of _sideband_ events. \n",
    "5. To plot normalized distribution of signal wrt variable TAU, one have to substract histogram  of background events (4) from histogram of _signal region_ events.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Checking correlation between MM & TAU\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "np.cov(df['D0_MM'],df['D0_TAU'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "np.cov(df['D0_MM'],df['D0_PT'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "np.cov(df['D0_MM'],df['D0_MINIP'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "seaborn.jointplot(df['D0_MM'],np.log(df['D0_TAU']*1000), kind='hex').\\\n",
    "  set_axis_labels(\"D0_MM\", \"log(D0_tau)\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Splitting into Signal and Sideband regions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "signal_region = (1844, 1890)\n",
    "signal_region_mask = df['D0_MM'] >= signal_region[0]\n",
    "signal_region_mask &= df['D0_MM'] < signal_region[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "signal_region_events = df[signal_region_mask]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sideband_region_mask = signal_region_mask == False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sideband_region_events = df[sideband_region_mask]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "back_scaling_factor = float(signal_region_events.shape[0]) / sideband_region_events.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "back_scaling_factor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# fig, (a1, a2) = plt.subplots(1, 2, figsize=(18, 6))\n",
    "a = seaborn.jointplot(signal_region_events['D0_MM'], signal_region_events['D0_TAU']*1000,\n",
    "                  kind='hex');\n",
    "a.ax_joint.set_title(\"aasd\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "seaborn.jointplot(sideband_region_events['D0_MM'],sideband_region_events['D0_TAU']*1000, kind='hex', color='r');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "n_bins = 30\n",
    "h_sr, bar_x, _ = plt.hist(signal_region_events['D0_TAU']*1000, bins=n_bins, histtype='stepfilled', alpha=0.5)\n",
    "h_sb, _, _ = plt.hist(sideband_region_events['D0_TAU']*1000/back_scaling_factor, bins=n_bins, histtype='step', alpha=1., lw=2, color='r')\n",
    "h_sig = h_sr - h_sb / back_scaling_factor\n",
    "plt.yscale('log')\n",
    "plt.xlabel('ps')\n",
    "plt.title('D0_TAU, Decay time');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.bar(bar_x[:-1], h_sig, width=bar_x[1] - bar_x[0])\n",
    "plt.yscale('log')\n",
    "plt.xlabel('ps')\n",
    "plt.title(\"Distribution Difference\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def line(x, m, c): # define it to be parabolic or whatever you like\n",
    "    return m * x + c\n",
    "err = np.maximum(np.nan_to_num(np.log(h_sig + np.sqrt(h_sig)) - np.log(h_sig)), 0.01)\n",
    "#err = np.ones(len(h_sig))\n",
    "chi2 = probfit.Chi2Regression(line, (bar_x[:1] + bar_x[1:])/2*1000, np.log(h_sig), err)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# plt.errorbar((bar_x[:1] + bar_x[1:])/2*1000, np.log(h_sig), yerr=err, fmt='none');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "minuit = iminuit.Minuit(chi2, m=-1, c=10, limit_m=(-5,0), error_m=0.01, error_c=0.1) # see iminuit tutorial on how to give initial value/range/error\n",
    "minuit.migrad();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "chi2.draw(minuit)\n",
    "print (\"D^{0} lifetime %.4f #pm %.6f (ps)\" % \n",
    "  (-1./minuit.values[\"m\"], minuit.errors[\"m\"]/(minuit.values[\"m\"]**2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tasks\n",
    "* MINIP dependence\n",
    "    - Repeat the previous steps (starting from Preselection) for different values of MINIP (from 10 to 0.01)\n",
    "    - Plot the trend of lifetime vs. MINIP threshold."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# TODO, make the plot described above\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Quiz\n",
    "\n",
    "* Does the Gaussian fit the mass distribution well?\n",
    "    - Explain the reason why in the fits where there is less background, the Gaussian undershoots the data points on the left.\n",
    "* Why does the original lifetime fit not agree with the PDG value?\n",
    "* Why does cutting on MINIP help, but cutting on D0 PT or TAU not help?\n",
    "* How would you estimate the systematic uncertainty on the measurement from the trend plot?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Put your answers below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
